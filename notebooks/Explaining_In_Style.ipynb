{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJxKQTz1Ycko"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataset import MultiResolutionDataset\n",
        "from torchvision import transforms, models, utils\n",
        "from model import Generator, Encoder\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import os\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import utils\n",
        "import requests\n",
        "import att_find_code.att_find_functions as att_find_func\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_z89LRUYckq"
      },
      "source": [
        "## Set the arguments for the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7NMDL3TYckr"
      },
      "outputs": [],
      "source": [
        "args = att_find_func.Args()\n",
        "args.output_path = \"att_find_artifacts\"\n",
        "args.ckpt = \"./output_cardio/checkpoint/045000.pt\"\n",
        "\n",
        "args.dataset_path = \"../Classifier/mdb/test\"\n",
        "args.classifier_ckpt = \"../Classifier/model_cardiomegaly.pth\"\n",
        "args.device = \"cuda\"\n",
        "args.batch = 16\n",
        "args.source = \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grkgDxysYcks"
      },
      "source": [
        "## Load the Generator, Encoder and Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXD9ctFfYcks"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(args.output_path):\n",
        "        os.mkdir(args.output_path)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "# load generator & encoder\n",
        "ckpt = torch.load(args.ckpt)\n",
        "params = ckpt[\"args\"]\n",
        "\n",
        "generator = Generator(params.size, params.latent, params.n_mlp, channel_multiplier=params.channel_multiplier,\n",
        "                conditional_gan=params.cgan if 'cgan' in params else False, \n",
        "                nof_classes=params.classifier_nof_classes if 'classifier_nof_classes' in params else False,\n",
        "                embedding_size=params.embedding_size).to(args.device)\n",
        "\n",
        "generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "generator.eval()\n",
        "\n",
        "# load classifier\n",
        "classifier = models.densenet121(pretrained=True)\n",
        "classifier.classifier = torch.nn.Linear(1024, 2)\n",
        "classifier.load_state_dict(torch.load(args.classifier_ckpt))\n",
        "classifier.to(args.device)\n",
        "classifier.eval()\n",
        "\n",
        "if args.source == \"data\":\n",
        "    encoder = Encoder(params.size, channel_multiplier=params.channel_multiplier, output_channels=params.latent\n",
        "    ).to(args.device)\n",
        "    encoder.load_state_dict(ckpt[\"e\"], strict=False)\n",
        "    encoder.eval()\n",
        "\n",
        "    if args.dataset_path is None:\n",
        "        print(\"Dataset path is required in data mode\")\n",
        "        exit()\n",
        "\n",
        "    # load dataset\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True),\n",
        "        ]\n",
        "    )\n",
        "    dataset = MultiResolutionDataset(args.dataset_path, transform, params.size, labels=True, filter_label=\"Pleural Effusion\")\n",
        "    args.n_sample = len(dataset)\n",
        "else:\n",
        "    dataset = None\n",
        "    encoder = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNxNFWLUYckt"
      },
      "outputs": [],
      "source": [
        "# get the parts of the generator that generate style\n",
        "affines = att_find_func.get_affines(generator)\n",
        "LAYER_SHAPES = att_find_func.get_layer_shapes(affines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWlU2HaGYckt"
      },
      "source": [
        "## Load the preprocessed data\n",
        "\n",
        "Loads and processes the data obtained by running att_find_calculate_styles.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW0Is3-VYcku"
      },
      "outputs": [],
      "source": [
        "# load the dictionary with all the preprocessed data\n",
        "loaded_features = torch.load(\"styles/features_style_cardio_no_finding_1128.pt\", map_location=\"cpu\")\n",
        "\n",
        "# load the dictionary with all the changes that led to classfier changing it's boundary\n",
        "# the keys are the style coordinate numbers, all the values are the list.\n",
        "# the list contatins the images, which were changed by the style coordinate and  and the change value\n",
        "change = torch.load(\"styles/change_cardio_no_finding_1128.pt\", map_location=\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZN-Im17Ycku"
      },
      "outputs": [],
      "source": [
        "# Preprocess loaded_features (only partially used for now)\n",
        "num_classes = 2\n",
        "style_change_effect = []\n",
        "dlatents = []\n",
        "base_probs = []\n",
        "labels = []\n",
        "for key in sorted(loaded_features.keys()):\n",
        "  feature = loaded_features[key]\n",
        "  dlatents.append(np.array(feature[\"dlatent\"]))\n",
        "  seffect = np.array(feature['result']).reshape((-1, 2, num_classes))\n",
        "  style_change_effect.append(seffect.transpose([1, 0, 2]))\n",
        "  base_probs.append(np.array(feature['base_prob']))\n",
        "  labels.append(np.array(feature['label']))\n",
        "style_change_effect = np.array(style_change_effect)\n",
        "dlatents = np.array(dlatents)\n",
        "labels = np.array(labels)\n",
        "W_values, style_change_effect, base_probs = dlatents, style_change_effect, np.array(base_probs)\n",
        "\n",
        "# style_change_effect = att_find_func.filter_unstable_images(style_change_effect, effect_threshold=4)\n",
        "dlatents_torch = torch.tensor(dlatents).to(args.device)\n",
        "\n",
        "all_style_vectors = torch.concat(att_find_func.get_style_for_dlantent(dlatents_torch, affines), axis=1).cpu().numpy()\n",
        "style_min = np.min(all_style_vectors, axis=0)\n",
        "style_max = np.max(all_style_vectors, axis=0)\n",
        "\n",
        "all_style_vectors_distances = np.zeros((all_style_vectors.shape[0], all_style_vectors.shape[1], 2))\n",
        "all_style_vectors_distances[:,:, 0] = all_style_vectors - np.tile(style_min, (all_style_vectors.shape[0], 1))\n",
        "all_style_vectors_distances[:,:, 1] = np.tile(style_max, (all_style_vectors.shape[0], 1)) - all_style_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9R0AchjYckv"
      },
      "outputs": [],
      "source": [
        "dlatents_tensor = torch.tensor(dlatents).to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk-mVdQEYckv"
      },
      "outputs": [],
      "source": [
        "# Select the most \"influential\" coordinates (that changed the most images)\n",
        "ch = {}\n",
        "for sindex, vals in change.items():\n",
        "    ch[sindex] = len(vals)\n",
        "    # print(sindex, \":\",len(vals))\n",
        "for i, (sindex, leng) in enumerate(sorted(ch.items(),key=lambda v:v[1], reverse=True)):\n",
        "    if leng < 10 or i > 10:\n",
        "        break  \n",
        "    print(sindex, \":\",leng, \"images changed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av6QMm2OYckv"
      },
      "outputs": [],
      "source": [
        "all_labels = np.argmax(base_probs, axis=1)\n",
        "style_effect_classes = {}\n",
        "W_classes = {}\n",
        "labels_classes = {}\n",
        "style_vectors_distances_classes = {}\n",
        "all_style_vectors_classes = {}\n",
        "label_size = 2\n",
        "for img_ind in range(label_size):\n",
        "  # print(img_ind)\n",
        "    img_inx = np.array([i for i in range(all_labels.shape[0]) \n",
        "    if all_labels[i] == img_ind])\n",
        "\n",
        "    labels_classes[img_ind] = all_labels[img_inx]\n",
        "    curr_style_effect = np.zeros((len(img_inx), style_change_effect.shape[1], \n",
        "                                  style_change_effect.shape[2], style_change_effect.shape[3]))\n",
        "    curr_w = np.zeros((len(img_inx), W_values.shape[1]))\n",
        "    curr_style_vector_distances = np.zeros((len(img_inx), style_change_effect.shape[2], 2))\n",
        "    for k, i in enumerate(img_inx):\n",
        "        curr_style_effect[k, :, :] = style_change_effect[i, :, :, :]\n",
        "        curr_w[k, :] = W_values[i, :]\n",
        "        curr_style_vector_distances[k, :, :] = all_style_vectors_distances[i, :, :]\n",
        "    style_effect_classes[img_ind] = curr_style_effect\n",
        "    W_classes[img_ind] = curr_w\n",
        "    style_vectors_distances_classes[img_ind] = curr_style_vector_distances\n",
        "    all_style_vectors_classes[img_ind] = all_style_vectors[img_inx]\n",
        "    print(f'Class {img_ind}, {len(img_inx)} images.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AytE2E8aYckw"
      },
      "outputs": [],
      "source": [
        "label_size_clasifier = 2 #@param\n",
        "num_indices =  18 #@param\n",
        "effect_threshold = 0.2 #@param\n",
        "use_discriminator = False #@param {type: 'boolean'}\n",
        "# discriminator_model = discriminator if use_discriminator else None\n",
        "s_indices_and_signs_dict = {}\n",
        "\n",
        "for class_index in [0, 1]:\n",
        "  split_ind = 1 - class_index\n",
        "  all_s = style_effect_classes[split_ind]\n",
        "  all_w = W_classes[split_ind]\n",
        "  # Find s indicies\n",
        "  s_indices_and_signs = att_find_func.find_significant_styles(\n",
        "    style_change_effect=all_s,\n",
        "    num_indices=num_indices,\n",
        "    class_index=class_index,\n",
        "    max_image_effect=effect_threshold*500,\n",
        "    sindex_offset=0)\n",
        "\n",
        "  s_indices_and_signs_dict[class_index] = s_indices_and_signs\n",
        "\n",
        "# Combine the style indicies for the two classes.\n",
        "sindex_class_0 = [sindex for _, sindex in s_indices_and_signs_dict[0]]\n",
        "\n",
        "all_sindex_joined_class_0 = [(1 - direction, sindex) for direction, sindex in \n",
        "                             s_indices_and_signs_dict[1] if sindex not in sindex_class_0]\n",
        "all_sindex_joined_class_0 += s_indices_and_signs_dict[0]\n",
        "\n",
        "scores = []\n",
        "for direction, sindex in all_sindex_joined_class_0:\n",
        "  other_direction = 1 if direction == 0 else 0\n",
        "  curr_score = np.mean(style_change_effect[:, direction, sindex, 0]) + np.mean(style_change_effect[:, other_direction, sindex, 1])\n",
        "  scores.append(curr_score)\n",
        "\n",
        "s_indices_and_signs = [all_sindex_joined_class_0[i] for i in np.argsort(scores)[::-1]]\n",
        "\n",
        "print('Directions and style indices for moving from class 1 to class 0 = ', s_indices_and_signs[:num_indices])\n",
        "print('Use the other direction to move for class 0 to 1.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McKmIPUXYckw"
      },
      "outputs": [],
      "source": [
        "#@title Visualize s-index {form-width: '20%'}\n",
        "import att_find_code.att_find_functions as att_find_func\n",
        "\n",
        "max_images = 10 #@param\n",
        "sindex =   2387#@param\n",
        "class_index = 1#@param {type: \"integer\"} \n",
        "shift_sign = \"1\" #@param [0, 1]\n",
        "wsign_index = int(shift_sign)\n",
        "print(\"Coordinate:\",sindex)\n",
        "if class_index == 0:\n",
        "  print(\"No finding\")\n",
        "else:\n",
        "  print(\"Cardiomegaly\")\n",
        "shift_size = 5#@param\n",
        "effect_threshold =  0.2#@param\n",
        "split_by_class = True #@param {type:\"boolean\"}\n",
        "select_images_by_s_distance = True #@param {type:\"boolean\"}\n",
        "draw_results_on_image = True #@param {type:\"boolean\"}\n",
        "\n",
        "if split_by_class:\n",
        "  split_ind = 1 if class_index == 0 else 0\n",
        "  all_s = style_effect_classes[split_ind]\n",
        "  all_w = W_classes[split_ind]\n",
        "  all_l = labels_classes[split_ind]\n",
        "  all_s_distances = style_vectors_distances_classes[split_ind]\n",
        "else:\n",
        "  all_s = style_change_effect\n",
        "  all_w = W_values\n",
        "  all_s_distances = all_style_vectors_distances\n",
        "\n",
        "additional_data_tuple = affines, LAYER_SHAPES, args.device\n",
        "\n",
        "font_file = '/tmp/arialuni.ttf'\n",
        "if not os.path.exists(font_file):\n",
        "  r = requests.get('https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/ipwn/arialuni.ttf')\n",
        "  open(font_file, 'wb').write(r.content)\n",
        "\n",
        "if not select_images_by_s_distance:\n",
        "  yy = visualize_style(generator, \n",
        "                       classifier,\n",
        "                       all_w,\n",
        "                       all_s,\n",
        "                       style_min,\n",
        "                       style_max,\n",
        "                       sindex,\n",
        "                       wsign_index,\n",
        "                       max_images=max_images,\n",
        "                       shift_size=shift_size,\n",
        "                       font_file=font_file,\n",
        "                       label_size=label_size,\n",
        "                       class_index=class_index,\n",
        "                       effect_threshold=effect_threshold,\n",
        "                       draw_results_on_image=draw_results_on_image)\n",
        "    \n",
        "else:\n",
        "  yy = att_find_func.visualize_style_by_distance_in_s(\n",
        "    generator,\n",
        "    classifier,\n",
        "    all_w,\n",
        "    all_l,\n",
        "    additional_data_tuple,\n",
        "    all_s_distances,\n",
        "    style_min,\n",
        "    style_max,\n",
        "    sindex,\n",
        "    wsign_index,\n",
        "    max_images=max_images,\n",
        "    shift_size=shift_size,\n",
        "    font_file=font_file,\n",
        "    label_size=label_size,\n",
        "    class_index=class_index,\n",
        "    effect_threshold=effect_threshold,\n",
        "    draw_results_on_image=draw_results_on_image)\n",
        "\n",
        "if yy.shape[0] > 0:\n",
        "  att_find_func.show_image(yy)\n",
        "else:\n",
        "  print('no images found')"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "366b680e80ffe50ee2314005add4e039ec38dfa27fa6c691fcb1338f272054ba"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('venv': virtualenv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Explaining_In_Style.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}